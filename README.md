# RT Estimation

This program calculates the relative positions of a set of cameras filming a scene with non-overlapping frames.
It uses an panoramic frame taken from an omnidirectional camera as a reference.
 
It has been implemented by Guillaume BEA and Florian LE VERN during their internship at __KEIO University__ in the __Hyper Vision Research Laboratory__ of Professor Hideo SAITO [\[link\]](http://www.hvrl.ics.keio.ac.jp/) from June to August 2016.

## Prerequisities

In order to use the program, some dependencies must be installed:

* __openCV__ _version 2_ [\[link\]](http://opencv.org/)  

* __openMVG__ _version 1.0_ [\[link\]](https://github.com/openMVG/openMVG) 

## Licenses

* __RT Estimation between omnidirectional and flat cameras__  
  Copyright &copy; 2016 Hyper Vision Research Laboratory, Shogo MIYATA, Guillaume BEA, Florian LE VERN.
  
* __Spherical Pose Solver__  
  Copyright &copy; 2012-2013 Pierre MOULON.
  Licensed under the [MPL2 license](http://opensource.org/licenses/MPL-2.0).

* __Open Source Computer Vision Library__  
  Copyright &copy; OpenCV authors.
  Licensed under the [3-Clause BSD License](http://opencv.org/license.html).

* __OpenMVG - open Multiple View Geometry__  
  Copyright &copy; OpenMVG authors.
  Licensed under the [MPL2 license](http://opensource.org/licenses/MPL-2.0).
 
## How to launch the program

First of all, build the program into a new folder:

```
$ mkdir build
$ (cd build && cmake ..)
$ (cd build && make)
```

The main action of this program is to take a set of pictures following a specific architecture in the file system and to calculate the Rotation and Translation matrices indicating the pose of these frames in the scene in the system of coordinates of the panoramic frame.

Thus, the first thing to do is to prepare your set of data.  
In a new folder that we will call `experiment`, create the following architecture:

```
- experiment
    - cam
        > Insert in this folder all the images 
          from the different cameras filming the scene 
          that you want their relative positions.
    - omni
        > Insert in this folder the panoramic image 
          of the scene. If there is more than one picture 
          in this folder, only the first one will be used. 
```

Then, the program can be launched with the following command (see the option section a bit farther below for more possibilities):

```
$ ./build/rt_estimation <path_to_experiment_folder> 
```

The calculated Rotation and Translation matrices will be saved into a `res.txt` file where the program has been launched.

## Options

The __RT Estimation__ program provides some options to allow several kind of experiments.

_-help or -h or -usage or -?_  
Print the help in the standard output.
    
_-manual or -m_  
Use the keypoints stored in the experiment folder (keypoints.yml) images instead of using SIFT. This file is generated by executing point_marker on an experiment (see next section).
    
_-reference=<image_name> or -ref=<image_name> or -r=<image_name>_  
Name of the image to use as the reference for coordinates instead of the panoramic image.

_-output=<filepath> or -o=<filepath>_  
Name of the output file containing (R,T) in the reference system of coordinates.

## Point marker

Another executable is generated after building the program. It can be used to manually select the matched keypoints, wich are stored in the experiment folder.

```
$ ./build/point_marker <path_to_experiment_folder> 
```

## Plot results

A python 3D viewer as been made in order to see the positions of the different cameras. Its input is the file generated by rt_estimation. Use the following command :

```
$ python plot/plot.py < res.txt 
```

## Sum up of the theory

Here is a brief description of the theory that this program uses:

In order to calculate the rotation and translation matrices, we use the panoramic image that sees all the scene as a reference. The issue is therefore to find the (R,T) couple between the omnidirectional image and the flat image.  

The idea is to use _feature detection_ and _feature matching_ to match some singular points between the two images. With this data, we are able to write _epipolar equations_ which solution will provide an _Essential matrix_.  

Then, this matrix can be decomposed into four possible (R,T) pairs. The selection of the unique solution uses the fact that only one pair shows all the singular 3D points _in front of both cameras_ (in reality we take the maximum).

